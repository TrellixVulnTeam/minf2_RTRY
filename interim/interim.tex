\documentclass[bsc,logo,twoside,singlespacing,parskip,deptreport]{infthesis}

\begin{document}

\title{Automatic Harmonic Analysis of Melodies}

\author{Finlay McAfee}

\course{Master of Informatics}
\project{{\bf MInf Project (Part 2) Interim Report}}

\date{\today}

\maketitle

%\pagenumbering{arabic}


\chapter{Introduction}
The aim of this project is to learn a model that can map a musical melodic sequence to to harmonic sequence. More specifically this model takes a temporal, monophonic sequence of notes and predicts the corresponding sequence of chords. These chords correspond to the harmonic accompaniment intended by the composer to be played along with the piece.

The primary assumption of this project is that it is useful to treat the melody as an observed sequence generated by the underlying chord sequence. In this sense we have a noisy channel model, similar to a natural language processing problem, where the note sequence can be likened to a sequence of words in a sentence and the chord sequence corresponds to the underlying meaning represented in these notes.

To this we can add the Markov Assumption. Specifically that the notes generated by a chord a time $t_i$ are conditionally independent of the notes generated by chords at all other times $t_j, i \neq j$, given the chord at $t_i$.

\begin{equation} \label{eq1}
P(c_t | n_t)
\end{equation}

This divides the problem into two distinct parts.

The first is modelling the generative process from chord to sequence of notes, here called the Emission Model. This model is very dependent on the frame of notes considered to be generated by a chord. This project works with two possible variants, either the Emission Model generates all the notes observed in the time frame associated with a certain chord, or we consider each note on it's own as independently being generated by a single chord instance and ignore the concept of frames. In the second case there will be a one-to-one correspondence between the chord sequence and the note sequence, and hence the former will consist of many repeated sections of chords.

DIAGRAM

The second model we must consider is the transitions between the chords, here on the Transition Model. Each chord can be thought of as a state, and hence every transition a movement between two states. Under a first order Markov Assumption, where we assume that the current state is only dependant on the previous one, then the transition model becomes a bigram model between chords, and the combined system becomes a Hidden Markov Model. Other possible transition models will be the focus of the remainder of this project. 

The data required for this project is symbolic melodic sequences annotated the accompanying chord. Hence this is not a system that is built to model raw sound data, neither is it an unsupervised model that can develop a notion of chords from unlabelled data. In the first year of this project data was used from the Weimar Jazz Database. This data consists of transcribed solos from jazz musicians over jazz standards. The data is heavily annotated and comes with chord labels drawn from the standards. However solos are a special subset of melodies with huge room for seemingly random improvisation and other forms of noise. This makes the problem significantly harder and hence one of the aims of this year was to find a dataset more suitable for the task and to further investigate the difficulties in using this data. 

\chapter{Year 1}

The focus in the first year of this project was on finding the best Emission Model to use. Three methods were developed, a 'Bag of Notes' model, a 'Chord Tone' model and a 'Concatenative HMM'. They were all combined with a simple bigram transition model and trained using the Viterbi algorithm.

\section{Bag of Notes}

The Bag of Notes model is analogous to a bag of words model, commonly applied to document analysis, where the frequencies of individual words are used to classify the document. Here the frequencies of notes in a frame are used to determine the chord. The training process creates $n$ smoothed and normalised frequency distributions over the $m$ possible notes, where $n$ is the number of chords being classified. 

Note that typically in this project $n = 12$ and we are training the model to classify the root note of the chord, as opposed to the particular variant of the chord i.e. whether it is major, minor or dominant. The number of possible notes, $m$, is typically also set to 12, where we are interested in Tonal Pitch Classes instead of absolute values. A Tonal Pitch Class (TPC) is an integer between 0 and 11, representing the relative separation between the note and the key of the song. Hence in the key of C, a D note would have a TPC of 2, and a C note would have a TPC of 0.

The failings of this model lie in the fact that it throws away sequential data and has no underlying musical intuition associated with it. The following model also discards local sequential data but is inspired by musical assumptions.

\section{Chord Tone Model}

The chords that are trying to be learned in this project are represented by root notes, but in reality are represented by a number of chord tones. In the case of majors and minors there 3 chord tones per chord, these are the root, the 3rd and the 5th. It can be safely assumed that in a melody over a chord, the chord tones of that particular chord are more likely to appear than the other notes. Hence a probabilistic model can that identify the chord tones in a melody has more information about the underlying chord than a blind frequency distribution.

Experiments were carried out to find the best method of determining which notes are more likely to be chord tones. The resulting model had two layers, each with a decision tree trained for classification. The first level took as input a feature vector representing a note. This vector had encoded metrical, structural and melodic information about the note. The dataset was modified to label which notes in the training set were chord tones, which the decision tree was then trained over. The second decision tree then took only the notes that were classified as chord tones and predicted the underlying chord. This layer was trained in a similar way, taking the chord tones in the training data as input and classifying the given chord labels. This two-layer system then formed the Emission Model and, together with the HMM Transition Model, used the Viterbi Algorithm to produce the most likely sequence of chords.

\section{Concatenative HMM}

The above model produced much more promising results than Bag of Notes, but it still disregards all local sequential data. Intuitively a lot of musical information is stored in the intervals between notes, and it was thought that a model that could capture this would perform better when determining the underlying chords. The Concatenative HMM attempts to capture this by training $n$ different HMMs, one for each chord, over frames of notes 'generated' by that chord. An observed sequence can then be run through these models, producing probabilities. These are then the emission probabilities for that sequence of observed notes, given the chord. Combining these with the Transition HMM model can be thought of as concatenating together a long list of smaller HMMs. 

There are two possible methods of training this model, supervised and unsupervised. As it is not the state sequence, but the probabilities that we are interested in for the lower level HMMs, the musical structure that the states would represent seems unclear. With an unsupervised approach, all that is necessary would be to define the number of states, then the Expectation Maximisation (EM) algorithm CITE can be used to learn the optimum state sequences for the training data. This approach will be discussed later in the report.

The other approach is to train in a supervised way and decide the representation of states manually based on musical knowledge. Again chord tones were used as a suitable middle ground between chords and notes, and the states of the low level HMMs were trained over labelled data, where each  note was annotated with chord tone information. In this model, however, multiple levels of chord tone were decided on, based on the intuition that the root and the 5th are more strongly related than the root and the 3rd, and that 7th is also used in some chords and could be considered a weaker form of chord tone. Hence multiple variants of this model were attempted, with different numbers of internal states for the HMMs.

\section{Implementation}

The language used for both years of this project in the implementation is Python. This choice was made based on a familiarity with the available libraries and a suitability for the domain of problem. Python is suitably high-level and well adept at processing scientific data.

The first part of this project involved an implementation of generalized HMMs for supervised learning, where the Emission Model and the Transition Model are modular, in the sense that the same HMM implementation could be used over the above three models, requiring only a class that could be trained and produced log probabilities given input data for the Emission Model. This was not readily available in pre-existing libraries. This involved implementation of the Viterbi algorithm.

The data was stored mainly in Python data structures and Numpy CITE arrays. Scikit-learn was used for cross-validation methods.

\section{Evaluation}

The evaluation of these models is subtly difficult and tends to be overly critical on the jazz corpus. This is due to the presence of jazz substitutions, where many different chords can be played in the place of one another for very little change in harmonic structure CITE. Nonetheless the clear way for evaluating this system is to test over a set of data not used in training and to compare the generated chords with the ground truth chords and create an accuracy metric. The results for the above models, classifying 12 chords, are presented in Table \ref{12}.

\begin{table}
\centering
\caption{Year 1 Results}
\label{12}
\begin{tabular}{l|l}
Model               & Accuracy (\%) \\ \hline
Bag of Notes HMM    & 22.5           \\
Concat HMM          & 30.7          \\
Chord Tone HMM (DT) & 27.5          \\
\end{tabular}
\end{table}

Against a baseline of random chance, 8.3\%, these models show some promise, but not as much as expected. These results promoted a more in-depth investigation into the data that will be detailed in the rest of this report. Another important discussion is the use of random choice as a baseline. This is overly generous to the models, a baseline that takes into account some basic music knowledge might be more enlightening as to the quality of both the models and the data.

\chapter{Year 2}

\section{Work Done So Far}

\section{Work Still To Do}

% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{bibfile}

\end{document}
