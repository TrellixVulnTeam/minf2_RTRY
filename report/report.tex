\documentclass[bsc,singlespacing,parskip, deptreport]{infthesis}

\begin{document}

\title{Automatic Harmonic Analysis of Melodies}

\author{Finlay McAfee - s1220880}

\course{Master of Informatics}
\project{{\bf MInf Project (Part 2) Report}}

\date{\today}

\abstract{
}

\maketitle

%\pagenumbering{arabic}
\chapter{Introduction}
\chapter{Background}
\chapter{Design and Implementation}
\chapter{Experimental Methodology and Datasets}
\chapter{Results}
\chapter{Conclusion}

\chapter{Interim Report Hereafter}
\chapter{Introduction}
The aim of this project is to learn a model that can map a musical melodic sequence to to harmonic sequence. More specifically this model takes a temporal, monophonic sequence of notes and predicts the corresponding sequence of chords. These chords correspond to the harmonic accompaniment intended by the composer to be played along with the piece.

The primary assumption of this project is that it is useful to treat the melody as an observed sequence generated by the underlying chord sequence. In this sense we have a noisy channel model, similar to a natural language processing problem, where the note sequence can be likened to a sequence of words in a sentence and the chord sequence corresponds to the underlying meaning represented in these notes.

To this we can add the Markov Assumption. Specifically that the notes generated by a chord a time $t_i$ are conditionally independent of the notes generated by chords at all other times $t_j, i \neq j$, given the chord at $t_i$.

\begin{equation} \label{eq1}
P(c_t | n_t) = P(n_t | c_t) P(c_t | c_{t-1})
\end{equation}

where $c_t$ is the chord at time $t$ and $n_t$ is the sequence of notes at time $t$.

This divides the problem into two distinct parts.

The first is modelling the generative process from chord to sequence of notes, here called the Emission Model. This model is very dependent on the frame of notes considered to be generated by a chord. This project works with two possible variants, either the Emission Model generates all the notes observed in the time frame associated with a certain chord, or we consider each note on it's own as independently being generated by a single chord instance and ignore the concept of frames. In the second case there will be a one-to-one correspondence between the chord sequence and the note sequence, and hence the former will consist of many repeated sections of chords.

The second model we must consider is the transitions between the chords, here on the Transition Model. Each chord can be thought of as a state, and hence every transition a movement between two states. Under a first order Markov Assumption, where we assume that the current state is only dependant on the previous one, then the transition model becomes a bigram model between chords, and the combined system becomes a Hidden Markov Model. Other possible transition models will be the focus of the remainder of this project. 

The data required for this project is symbolic melodic sequences annotated the accompanying chord. Hence this is not a system that is built to model raw sound data, neither is it an unsupervised model that can develop a notion of chords from unlabelled data. In the first year of this project data was used from the Weimar Jazz Database. This data consists of transcribed solos from jazz musicians over jazz standards. The data is heavily annotated and comes with chord labels drawn from the standards. However solos are a special subset of melodies with huge room for seemingly random improvisation and other forms of noise. This makes the problem significantly harder and hence one of the aims of this year was to find a dataset more suitable for the task and to further investigate the difficulties in using this data. 

\chapter{Year 1}

The focus in the first year of this project was on finding the best Emission Model to use. Three methods were developed, a 'Bag of Notes' model, a 'Chord Tone' model and a 'Concatenative HMM'. They were all combined with a simple bigram transition model and trained using the Viterbi algorithm.

\section{Bag of Notes}

The Bag of Notes model is analogous to a bag of words model, commonly applied to document analysis, where the frequencies of individual words are used to classify the document. Here the frequencies of notes in a frame are used to determine the chord. The training process creates $n$ smoothed and normalised frequency distributions over the $m$ possible notes, where $n$ is the number of chords being classified. 

Note that typically in this project $n = 12$ and we are training the model to classify the root note of the chord, as opposed to the particular variant of the chord i.e. whether it is major, minor or dominant. The number of possible notes, $m$, is typically also set to 12, where we are interested in Tonal Pitch Classes instead of absolute values. A Tonal Pitch Class (TPC) is an integer between 0 and 11, representing the relative separation between the note and the key of the song. Hence in the key of C, a D note would have a TPC of 2, and a C note would have a TPC of 0.

The failings of this model lie in the fact that it throws away sequential data and has no underlying musical intuition associated with it. The following model also discards local sequential data but is inspired by musical assumptions.

\section{Chord Tone Model}

The chords that are trying to be learned in this project are represented by root notes, but in reality are represented by a number of chord tones. In the case of majors and minors there 3 chord tones per chord, these are the root, the 3rd and the 5th. It can be safely assumed that in a melody over a chord, the chord tones of that particular chord are more likely to appear than the other notes. Hence a probabilistic model can that identify the chord tones in a melody has more information about the underlying chord than a blind frequency distribution.

Experiments were carried out to find the best method of determining which notes are more likely to be chord tones. The resulting model had two layers, each with a decision tree trained for classification. The first level took as input a feature vector representing a note. This vector had encoded metrical, structural and melodic information about the note. The dataset was modified to label which notes in the training set were chord tones, which the decision tree was then trained over. The second decision tree then took only the notes that were classified as chord tones and predicted the underlying chord. This layer was trained in a similar way, taking the chord tones in the training data as input and classifying the given chord labels. This two-layer system then formed the Emission Model and, together with the HMM Transition Model, used the Viterbi Algorithm to produce the most likely sequence of chords.

\section{Concatenative HMM}

The above model produced much more promising results than Bag of Notes, but it still disregards all local sequential data. Intuitively a lot of musical information is stored in the intervals between notes, and it was thought that a model that could capture this would perform better when determining the underlying chords. The Concatenative HMM attempts to capture this by training $n$ different HMMs, one for each chord, over frames of notes 'generated' by that chord. An observed sequence can then be run through these models, producing probabilities. These are then the emission probabilities for that sequence of observed notes, given the chord. Combining these with the Transition HMM model can be thought of as concatenating together a long list of smaller HMMs. 

There are two possible methods of training this model, supervised and unsupervised. As it is not the state sequence, but the probabilities that we are interested in for the lower level HMMs, the musical structure that the states would represent seems unclear. With an unsupervised approach, all that is necessary would be to define the number of states, then the Expectation Maximisation (EM) algorithm can be used to learn the optimum state sequences for the training data. This approach will be discussed later in the report.

The other approach is to train in a supervised way and decide the representation of states manually based on musical knowledge. Again chord tones were used as a suitable middle ground between chords and notes, and the states of the low level HMMs were trained over labelled data, where each  note was annotated with chord tone information. In this model, however, multiple levels of chord tone were decided on, based on the intuition that the root and the 5th are more strongly related than the root and the 3rd, and that 7th is also used in some chords and could be considered a weaker form of chord tone. Hence multiple variants of this model were attempted, with different numbers of internal states for the HMMs.

\section{Implementation}

The language used for both years of this project in the implementation is Python. This choice was made based on a familiarity with the available libraries and a suitability for the domain of problem. Python is suitably high-level and well adept at processing scientific data.

The first part of this project involved an implementation of generalized HMMs for supervised learning, where the Emission Model and the Transition Model are modular, in the sense that the same HMM implementation could be used over the above three models, requiring only a class that could be trained and produced log probabilities given input data for the Emission Model. This was not readily available in pre-existing libraries. This involved implementation of the Viterbi algorithm.

The data was stored mainly in Python data structures and Numpy arrays. Scikit-learn was used for cross-validation methods.

\section{Evaluation}

The evaluation of these models is subtly difficult and tends to be overly critical on the jazz corpus. This is due to the presence of jazz substitutions, where many different chords can be played in the place of one another for very little change in harmonic structure. Nonetheless the clear way for evaluating this system is to test over a set of data not used in training and to compare the generated chords with the ground truth chords and create an accuracy metric. The results for the above models, classifying 12 chords, are presented in Table \ref{12}.

\begin{table}
\centering
\caption{Year 1 Results}
\label{12}
\begin{tabular}{l|l}
Model               & Accuracy (\%) \\ \hline
Bag of Notes HMM    & 22.5           \\
Concat HMM          & 30.7          \\
Chord Tone HMM (DT) & 27.5          \\
\end{tabular}
\end{table}

Against a baseline of random chance, 8.3\%, these models show some promise, but not as much as expected. These results promoted a more in-depth investigation into the data that will be detailed in the rest of this report. Another important discussion is the use of random choice as a baseline. This is overly generous to the models, a baseline that takes into account some basic music knowledge might be more enlightening as to the quality of both the models and the data.

\chapter{Year 2}

\section{Work Done So Far}
\subsection{New Data}
The first task faced in the second year of this project was to find a new dataset. This would allow for a more robust analysis of the previous year's models and the opportunity to develop better models this year.

A remark should be made about the difficulty in collecting data in this field. There exist many private collections of song data annotated for this purpose, some with the intention of being made public, but copyright laws cause this to be very difficult in practice. Of the data that does exist in the public domain, much of it is either melodic or chord data, rarely are the two aligned for the purposes required by this task. The Weimar Jazz Database is a superb resource, it is unfortunate that harmonic analysis of data in the specific domain of jazz solos turns out to be very difficult.

After investigation, the most promising new source of data appeared to be the KP Corpus of annotated classical melodies. It consists of midi versions of 46 classical excerpts, annotated with chord names.

The next step was to write a midi parser that would transform the data into Numpy arrays. MIDICSV was used to convert the data into csv format then Python's csv library was used to parse the data into Numpy. From here the models used last year were re-implemented. The results are presented in \ref{kp}.

\begin{table}
\centering
\caption{KP Corpus}
\label{kp}
\begin{tabular}{l|l}
Model               & Accuracy (\%) \\ \hline
Bag of Notes HMM    & 3.4           \\
Concat HMM          & 21.7          \\
Chord Tone HMM (DT) & 19.7          \\
\end{tabular}
\end{table} 

Although the data in this corpus is much more suited to the task, there is much less data than the Weimar Jazz Database, so the models end up performing slightly worse. 
 
\subsection{Unsupervised HMMs}

The Concatenative HMM model used supervised HMMs as the Emission Model, as described in Section 2.3. Part of the continued work of this year was to reimplement this model with unsupervised HMMs, trained using the Expectation Maximisation (EM) algorithm. 

The HMMs used previously were implemented from scratch, to allow for generality, but for these unsupervised models the hmmlearn python library was used. The notes were encoded into one-hot vectors and $n$ Multinomial HMMs were trained, one for each chord.



\subsection{Unframed HMMs}

So far every model in this project has processed the melody into frames, with one chord 'generating' a small sequence of notes. Hence the Emission Model has been a sequential probabilistic model and the Transition Model has been a simple matrix of transition probabilities between chords.

For the next model we will alter our assumptions slightly, so that instead there is a one-to-one correspondence between notes and chords. Hence the data is modified so that the labels consist of repeated instances of each chord. This translates to many self-transitions between states and a simple chord-to-note Emission Model.

In this way the entire system can be represented as an HMM with multinomial emissions, where the notes are again encoded as one-hot vectors.

This model surprisingly out performed all others on the KP corpus, achieving an accuracy of 51.04\%. Interestingly, the same model trained on the Weimar Jazz Database predicts only the tonic chord every time, scoring 32\% accuracy. This can be taken as evidence that the KP corpus is better suited to the task of chord prediction. 

\section{Work Still To Do}

\subsection{Data Analysis}

The next task to be completed will be a deeper analysis of the songs from both corpora, to confirm the intuitions drawn from working with both and the results of the models.

This will involve manual analysis of individual songs, where I will attempt to perform the task being carried out by these models to determine the inherent difficulties associated with each corpus. This will also help to inform an understanding of how the models will capture the associations between melodies and chords. 

\subsection{Baselines}

The baseline chosen to compare the models to was one of random chance, i.e. $1/n$ where $n$ is the number of chords. This is overly simplistic and hence two more baselines will be implemented as models for comparison.

The first will determine the tonic chord and guess this every time. Note that this is what the unframed HMM did in practice on the Weimar Jazz Database. This may be an overly harsh baseline, as it is quite hard to beat (the tonic chord is almost always the most common, and most melodies in a key will fit over the tonic).

The second baseline will take the starting note of the song as the root of the chord that it guesses every time. This seems a fairer baseline as it inherently models less information about the whole structure of the song.

\subsection{Sequence to Sequence Model}

The final step in this project will be to implement a model that differs quite significantly in its assumptions from the others. This will be a sequence to sequence model, consisting of an encoder and a decoder. The encoder will take the note sequence of the song and encode it into a fixed dimension vector. The decoder will then take this vector and decode it into a sequence of chords. These two models will be Recurrent Neural Networks (RNN), implemented in tensorflow. The RNNs used will be a basic RNN Cell, a Long Short Term Memory Cell (LSTM) and a Gated Recurrent Unit Cell (GRU).




% use the following and \cite{} as above if you use BibTeX
% otherwise generate bibtem entries
\bibliographystyle{plain}
\bibliography{bibfile}

\end{document}
